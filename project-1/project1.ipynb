{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "we will start with Data Collection first from Kaggle and as we have imported the required Data we shall then move on to EDA\n",
    "\n",
    "During EDA(Exploratory Data Analytics) and this is a neccessary step towards a Machine Learning project since it helps with understanding the data and analyzing what models can be used to work on the data.\n",
    "\n",
    "Then we move on to Data PreProcessing where we perform under sampling and over sampling.\n",
    "\n",
    "Train Test Split - here we train and test the data\n",
    "Then work with tree Models like random forest, xtree boost classifier, etc. as well as use cross validation.\n",
    "\n",
    "Find the best trained model\n",
    "Then send unknown data and predict of our required goal i.e to find out if a customer will be leaving or not.\n",
    "\n",
    "Then we study the dataset and analyze what are the columns shown in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we download the required pytohn libraries for the prediction project\n",
    " do this by running these commands individually\n",
    "NOTE: run each command only after the previous has stopped running.\n",
    "Commands:\n",
    "\n",
    "pip install pandas \\n\n",
    "\n",
    "pip install numpy \n",
    "\n",
    "pip install scikit-learn\n",
    "\n",
    "pip install matplotlib \n",
    "\n",
    "pip install xgboost\n",
    "after this restart the python  kernel being used in the notebook.>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from imblearn import SMOTE #for oversampling technique to build a uniformly distributed target class\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split #from splitting data into train test and split\n",
    "from imblearn.over_sampling import SMOTE #to solve class imbalance issues\n",
    "\n",
    "# lets import few decision tree based models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7043, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we load the csv data to a pandas DataFrame\n",
    "df= pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "df.shape #this shows the number of rows and columns we have in our DataSet and this is a decent number of of data we have for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None) #this command ensures all columns are visible in the DataSet\n",
    "df.head #this prints the first 5 columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
